#!/usr/bin/env python3
"""
PyTorch LSTM ê¸°ë°˜ ì‹¤ì‹œê°„ ëª¨ë“  UE ìœ„ì¹˜ ì¶”ì •
ğŸ§  ì‹œê³„ì—´ SINR ë°ì´í„°ë¥¼ í™œìš©í•œ ì •í™•í•œ ìœ„ì¹˜ ì˜ˆì¸¡

ì£¼ìš” íŠ¹ì§•:
- Bidirectional LSTM ëª¨ë¸ ì‚¬ìš©
- ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ (LOOKBACK=10)
- ì‹¤ì‹œê°„ ì²˜ë¦¬ ë° ì¦‰ì‹œ ì €ì¥
- ëª¨ë“  UE ë™ì‹œ ìœ„ì¹˜ ì¶”ì •
"""

import numpy as np
import torch
import torch.nn as nn
import joblib
import queue
import signal
import sys
import time
from collections import defaultdict, deque
from sklearn.preprocessing import StandardScaler
import torch.nn.functional as F  # â† ì´ê²Œ ì—†ìŒ!

# utils.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ import
from utils_Standard import (
    SINRMeasurement, 
    DataParser,
    setup_logging, 
    create_socket_receiver, 
    start_receiver_thread
)

CONFIG = {
    'model_path': 'model/lstm_3gpp.pth',           
    'scaler_x_path': 'model/lstm_3gpp_x.pkl',      # ğŸ”¥ StandardScaler
    'scaler_y_path': 'model/lstm_3gpp_y.pkl',      # ğŸ”¥ StandardScaler
    'output_file': 'lstm_trajectory_3gpp.txt',
    'lookback': 10,
    'input_size': 7,      
    'hidden_size': 128,
    'num_layers': 3,      
    'output_size': 2,
    'max_speed': 5.0,
    'time_interval': 0.1
}

class UELocalizationLSTM(nn.Module):
    """í•™ìŠµ ì½”ë“œì™€ ì •í™•íˆ ë™ì¼í•œ LSTM êµ¬ì¡°"""
    
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(UELocalizationLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True, 
            bidirectional=True,
            dropout=0.2 if num_layers > 1 else 0  
        )
        
        self.fc1 = nn.Linear(hidden_size * 2, 32)
        self.fc2 = nn.Linear(32, output_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]
        out = F.relu(self.fc1(last_output))  
        out = self.dropout(out)
        out = self.fc2(out)
        return out

class TimeSeriesBuffer:
    """LSTMìš© ì‹œê³„ì—´ ë°ì´í„° ë²„í¼"""
    
    def __init__(self, max_length: int = 10):
        self.max_length = max_length
        self.buffer = deque(maxlen=max_length)
        
    def add_features(self, features: np.ndarray):
        """ìƒˆë¡œìš´ íŠ¹ì„±ì„ ë²„í¼ì— ì¶”ê°€"""
        self.buffer.append(features.copy())
        
    def is_ready(self) -> bool:
        """ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ì¤€ë¹„ ì—¬ë¶€"""
        return len(self.buffer) == self.max_length
    
    def get_sequence(self) -> np.ndarray:
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°˜í™˜ [sequence_length, features]"""
        return np.array(list(self.buffer))
    
    def get_length(self) -> int:
        """í˜„ì¬ ë²„í¼ ê¸¸ì´"""
        return len(self.buffer)

class LSTMLocalizer:
    """LSTM ê¸°ë°˜ ì‹¤ì‹œê°„ ìœ„ì¹˜ ì¶”ì •ê¸°"""
    
    def __init__(self, model_path: str, scaler_x_path: str, scaler_y_path: str, output_file: str = "lstm_trajectory.txt"):
        print(f"ğŸ§  Initializing LSTM Localizer for ALL UEs...")
        
        # Device ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
        # LSTM ëª¨ë¸ ë¡œë“œ
        self.model = self._load_lstm_model(model_path)
        
        # Scaler ë¡œë“œ
        self.scaler_x = self._load_scaler(scaler_x_path, "Input")
        self.scaler_y = self._load_scaler(scaler_y_path, "Output")
        
        # ì¶œë ¥ íŒŒì¼ ì„¤ì •
        self.output_file = output_file
        self.file_handle = self._init_output_file()
        
        # UEë³„ ì‹œê³„ì—´ ë²„í¼ (LOOKBACK ê°œì˜ ê³¼ê±° ë°ì´í„° ì €ì¥)
        self.ue_buffers = defaultdict(lambda: TimeSeriesBuffer(CONFIG['lookback']))
        self.ue_previous_positions = {}
        # í†µê³„ ë° ìƒíƒœ
        self.stats = {'processed': 0, 'estimated': 0, 'start_time': None}
        self.running = False

        # ğŸ”¥ ì¶”ê°€: ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë¡œë“œ
        try:
            self.category_mappings = joblib.load("category_mappings.pkl")
            print(f"âœ… Category mappings loaded:")
            for cat, mapping in self.category_mappings.items():
                print(f"  {cat}: {mapping}")
        except FileNotFoundError:
            print("âŒ category_mappings.pkl not found! Using default mapping")
            self.category_mappings = None

        print(f"ğŸ¯ LSTM Localizer ready for ALL UEs")
        print(f"ğŸ’¾ Output file: {self.output_file}")
        print(f"ğŸ”¢ Sequence length: {CONFIG['lookback']}")   
    
    def _init_output_file(self):
        """ì¶œë ¥ íŒŒì¼ ì´ˆê¸°í™”"""
        file_handle = open(self.output_file, 'w', buffering=1)
        file_handle.write("timestamp,imsi,x,y\n")
        file_handle.flush()
        print(f"ğŸ“ Output file initialized: {self.output_file}")
        return file_handle
    
    def _load_lstm_model(self, model_path: str):
        """PyTorch LSTM ëª¨ë¸ ë¡œë“œ"""
        try:
            # ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            # ëª¨ë¸ ìƒì„±
            model = UELocalizationLSTM(
                input_size=CONFIG['input_size'],
                hidden_size=CONFIG['hidden_size'],
                output_size=CONFIG['output_size'],
                num_layers=CONFIG['num_layers']
            ).to(self.device)
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()  # ì¶”ë¡  ëª¨ë“œ
            
            print(f"âœ… LSTM model loaded from {model_path}")
            print(f"ğŸ§  Model parameters: {sum(p.numel() for p in model.parameters()):,}")
            
            return model
            
        except FileNotFoundError:
            print(f"âŒ Model file not found: {model_path}")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ Error loading LSTM model: {e}")
            sys.exit(1)
    
    def _load_scaler(self, scaler_path: str, scaler_type: str):
        """Scaler íŒŒì¼ ë¡œë“œ"""
        try:
            scaler = joblib.load(scaler_path)
            print(f"âœ… {scaler_type} scaler loaded from {scaler_path}")
            return scaler
        except FileNotFoundError:
            print(f"âŒ {scaler_type} scaler file not found: {scaler_path}")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ Error loading {scaler_type} scaler: {e}")
            sys.exit(1)
    
    def _extract_features(self, measurement: SINRMeasurement) -> np.ndarray:
        """í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ 6ê°œ features ì¶”ì¶œ (UE IDëŠ” featureì—ì„œ ì œì™¸)"""
                
        # 2. í•™ìŠµê³¼ ì™„ì „íˆ ë™ì¼í•œ 7ê°œ featuresë§Œ ì¶”ì¶œ
        features = [
            measurement.timestamp,                  # relative_timestamp
            measurement.serving_cell_x,          # serving_x
            measurement.serving_cell_y,          # serving_y  
            measurement.serving_cell_sinr,       # L3 serving SINR 3gpp_ma
            measurement.neighbor1_sinr,          # L3 neigh SINR 3gpp 1 (convertedSinr)_ma
            measurement.neighbor2_sinr,          # L3 neigh SINR 3gpp 2 (convertedSinr)_ma
            measurement.neighbor3_sinr           # L3 neigh SINR 3gpp 3 (convertedSinr)_ma
        ]
        
        # ğŸ” ë””ë²„ê¹…ìš© (ì²˜ìŒ ëª‡ ë²ˆë§Œ ì¶œë ¥)
        if measurement.ue_id not in getattr(self, '_debug_printed', set()):
            if not hasattr(self, '_debug_printed'):
                self._debug_printed = set()
            
            print(f"ğŸ§  UE_{measurement.ue_id} features: "
                f"ts={measurement.timestamp:.1f}, "
                f"pos=({measurement.serving_cell_x},{measurement.serving_cell_y}), "
                f"sinr=[{measurement.serving_cell_sinr:.1f}, "
                f"{measurement.neighbor1_sinr:.1f}, "
                f"{measurement.neighbor2_sinr:.1f}, "
                f"{measurement.neighbor3_sinr:.1f}]")
            
            self._debug_printed.add(measurement.ue_id)
        
        return np.array(features, dtype=np.float32)
    
    def apply_movement_constraint(self, pred_pos: np.ndarray, prev_pos: np.ndarray, 
                            max_speed: float = 5.0, dt: float = 0.1) -> np.ndarray:

        max_distance = max_speed * dt  # ìµœëŒ€ ì´ë™ê±°ë¦¬ (ì˜ˆ: 5m/s * 0.1s = 0.5m)
        
        # ì´ë™ ë²¡í„° ê³„ì‚°
        movement = pred_pos - prev_pos
        distance = np.linalg.norm(movement)
        
        # ìµœëŒ€ ì´ë™ê±°ë¦¬ ì´ˆê³¼ ì‹œ ì œí•œ
        if distance > max_distance:
            # ë°©í–¥ì€ ìœ ì§€í•˜ë˜ ê±°ë¦¬ë§Œ ì œí•œ
            movement = movement * (max_distance / distance)
            constrained_pos = prev_pos + movement
            
            print(f"âš ï¸  UE_{ue_id} Movement constrained: {distance:.2f}m â†’ {max_distance:.2f}m")
            return constrained_pos
        
        return pred_pos

    def process_measurement(self, measurement: SINRMeasurement):
        """ğŸ§  LSTMìœ¼ë¡œ ìœ„ì¹˜ ì¶”ì • (UE IDëŠ” ë²„í¼ í‚¤ë¡œë§Œ ì‚¬ìš©)"""
        try:
            # 1. íŠ¹ì„± ì¶”ì¶œ (6ê°œ features, UE ID ì œì™¸)
            features = self._extract_features(measurement)
            
            # 2. UE IDë¥¼ í‚¤ë¡œ ì‚¬ìš©í•´ì„œ UEë³„ ì‹œê³„ì—´ ë²„í¼ì— ì¶”ê°€
            buffer = self.ue_buffers[measurement.ue_id]  # â† UE IDëŠ” ì—¬ê¸°ì„œë§Œ ì‚¬ìš©
            buffer.add_features(features)
            
            # 3. ì¶©ë¶„í•œ ì‹œê³„ì—´ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì˜ˆì¸¡
            if buffer.is_ready():
                # ì‹œê³„ì—´ ë°°ì—´ ìƒì„± [LOOKBACK, 6 Features]
                sequence = buffer.get_sequence()
                
                # ì…ë ¥ ì •ê·œí™” (6ê°œ features)
                sequence_scaled = self.scaler_x.transform(
                    sequence.reshape(-1, 7)  # âœ… 7ê°œë¡œ ìˆ˜ì •
                ).reshape(sequence.shape)
                
                # PyTorch í…ì„œë¡œ ë³€í™˜ [1, LOOKBACK, 6]
                sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(self.device)
                
                # LSTM ì˜ˆì¸¡
                with torch.no_grad():
                    prediction_scaled = self.model(sequence_tensor)
                
                # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
                prediction_scaled_np = prediction_scaled.cpu().numpy()
                
                # ì¶œë ¥ ì—­ì •ê·œí™”
                raw_prediction = self.scaler_y.inverse_transform(prediction_scaled_np)[0]
                
                # ğŸ”¥ ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ ì ìš©
                if measurement.ue_id in self.ue_previous_positions:
                    prev_pos = self.ue_previous_positions[measurement.ue_id]
                    constrained_prediction = self.apply_movement_constraint(raw_prediction, prev_pos)
                else:
                    constrained_prediction = raw_prediction
                
                # ì´ì „ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
                self.ue_previous_positions[measurement.ue_id] = constrained_prediction.copy()
                
                # x, y ì¢Œí‘œ ì¶”ì¶œ
                x, y = constrained_prediction

                # íŒŒì¼ì— ì¦‰ì‹œ ì €ì¥ (UE ID í¬í•¨)
                self.file_handle.write(f"{measurement.timestamp},{measurement.ue_id},{x:.6f},{y:.6f}\n")
                
                self.stats['estimated'] += 1
                
                # ì£¼ê¸°ì  ì¶œë ¥ì—ì„œë„ UE ID ì‚¬ìš©
                if self.stats['estimated'] % 10 == 0:
                    print(f"ğŸ§  UE_{measurement.ue_id}: ({x:.2f},{y:.2f}) | "
                        f"RelTime:{measurement.timestamp:.1f}ms | "
                        f"SINR: [{measurement.serving_cell_sinr:.1f}, {measurement.neighbor1_sinr:.1f}] | "
                        f"Buffer: {buffer.get_length()}/{CONFIG['lookback']}")
                        
            else:
                # ë²„í¼ ì±„ìš°ëŠ” ì¤‘
                if buffer.get_length() % 5 == 0:
                    print(f"ğŸ“Š UE_{measurement.ue_id}: Buffering {buffer.get_length()}/{CONFIG['lookback']}...")
                    
        except Exception as e:
            print(f"âŒ LSTM prediction error for UE {measurement.ue_id}: {e}")
    
    def start_data_collection(self, data_queue: queue.Queue):
        """ğŸ”¥ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ëª¨ë“  UE ì²˜ë¦¬"""
        self.running = True
        self.stats['start_time'] = time.time()
        print(f"ğŸš€ Starting real-time LSTM localization for ALL UEs")
        print(f"â±ï¸  Sequence buffer size: {CONFIG['lookback']}")
        
        while self.running:
            try:
                line = data_queue.get(timeout=0.1)
                measurement = DataParser.parse_sinr_line(line)
                
                if measurement:  # ëª¨ë“  UE ì²˜ë¦¬
                    self.stats['processed'] += 1
                    
                    # ëª¨ë“  UEì— ëŒ€í•´ LSTM ìœ„ì¹˜ ì¶”ì •
                    self.process_measurement(measurement)
                    
                    # ì£¼ê¸°ì  ìƒíƒœ ì¶œë ¥
                    if self.stats['processed'] % 100 == 0:
                        self._print_status()
                        
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ Real-time processing error: {e}")
    
    def _print_status(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.stats['start_time'] if self.stats['start_time'] else 0
        processing_rate = self.stats['processed'] / elapsed if elapsed > 0 else 0
        success_rate = (self.stats['estimated'] / self.stats['processed']) * 100 if self.stats['processed'] > 0 else 0
        
        # âœ… ì¡´ì¬í•˜ëŠ” ì†ì„±ìœ¼ë¡œ ìˆ˜ì •
        unique_ues = len(self.ue_buffers)  # ue_buffersëŠ” ì¡´ì¬í•¨
        active_buffers = sum(1 for buffer in self.ue_buffers.values() if buffer.is_ready())

        print(f"ğŸ“Š LSTM ALL UEs | "
            f"Rate:{processing_rate:.1f}/s | "
            f"Success:{success_rate:.1f}% | "
            f"Processed:{self.stats['processed']} | "
            f"Estimated:{self.stats['estimated']} | "
            f"UEs:{unique_ues} | "
            f"Ready:{active_buffers}")
    
    def stop(self):
        """ì •ì§€ ë° íŒŒì¼ ì•ˆì „ ì¢…ë£Œ"""
        print(f"\nğŸ›‘ Stopping LSTM localization...")
        self.running = False
        
        # ìµœì¢… í†µê³„
        self._print_status()
        print(f"\nğŸ“ˆ Final UE Buffer Status:")
        for ue_id, buffer in self.ue_buffers.items():
            status = buffer.get_length()
            ready = "âœ… Ready" if buffer.is_ready() else "â³ Buffering"
            print(f"   UE_{ue_id}: {status}/{CONFIG['lookback']} measurements - {ready}")
        
        # íŒŒì¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
        if self.file_handle:
            self.file_handle.flush()
            self.file_handle.close()
            print(f"âœ… Output file closed: {self.output_file} ({self.stats['estimated']} total positions)")
        
        print(f"âœ… LSTM localization completed")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    
    print("=" * 60)
    print("ğŸ§  PyTorch LSTM-Based Real-time ALL UEs Localization")
    print("ğŸ”„ Time-series SINR pattern learning")
    print("=" * 60)
    print(f"ğŸ¯ Target: ALL UEs (no filtering)")
    print(f"ğŸ§  Model: {CONFIG['model_path']}")
    print(f"ğŸ“Š Input scaler: {CONFIG['scaler_x_path']}")
    print(f"ğŸ“Š Output scaler: {CONFIG['scaler_y_path']}")
    print(f"ğŸ’¾ Output file: {CONFIG['output_file']}")
    print(f"ğŸ”¢ Sequence length: {CONFIG['lookback']} timesteps")
    print("=" * 60)
    
    # LSTM ìœ„ì¹˜ ì¶”ì •ê¸° ì´ˆê¸°í™”
    try:
        localizer = LSTMLocalizer(
            model_path=CONFIG['model_path'],
            scaler_x_path=CONFIG['scaler_x_path'],
            scaler_y_path=CONFIG['scaler_y_path'],
            output_file=CONFIG['output_file']
        )
    except Exception as e:
        print(f"âŒ LSTM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì‹  ì„¤ì • (utils.py í™œìš©)
    data_queue = queue.Queue()
    receiver = create_socket_receiver(data_queue)
    receiver_thread = start_receiver_thread(receiver)
    
    # ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬
    def signal_handler(sig, frame):
        print(f"\nğŸ›‘ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹  ({sig})")
        localizer.stop()
        receiver.stop()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    print("ğŸš€ LSTM localization started for ALL UEs!")
    print("ğŸ’¡ C xAppì„ ì‹¤í–‰í•´ì„œ SINR ë°ì´í„°ë¥¼ ì „ì†¡í•˜ì„¸ìš”")
    print("ğŸ”„ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµìœ¼ë¡œ ì •í™•í•œ ìœ„ì¹˜ ì˜ˆì¸¡!")
    print(f"â±ï¸  {CONFIG['lookback']}ê°œ ì¸¡ì •ê°’ì´ ëª¨ì´ë©´ ì˜ˆì¸¡ ì‹œì‘")
    print("ğŸ›‘ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
    print("-" * 60)
    
    try:
        # ğŸ§  ì‹¤ì‹œê°„ LSTM ìœ„ì¹˜ ì¶”ì • ì‹œì‘
        localizer.start_data_collection(data_queue)
    except KeyboardInterrupt:
        pass
    finally:
        print("\nğŸ›‘ ì •ë¦¬ ì¤‘...")
        localizer.stop()
        receiver.stop()
        print("âœ… LSTM localization ì™„ë£Œ!")

if __name__ == "__main__":
    print("ğŸ” Checking required files...")
    
    required_files = [
        CONFIG['model_path'],
        CONFIG['scaler_x_path'], 
        CONFIG['scaler_y_path']
    ]
    
    missing_files = []
    for file_path in required_files:
        try:
            with open(file_path, 'rb'):
                pass
            print(f"  âœ… {file_path}")
        except FileNotFoundError:
            missing_files.append(file_path)
            print(f"  âŒ {file_path}")
    
    if missing_files:
        print(f"\nâŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤:")
        for file in missing_files:
            print(f"   - {file}")
        print(f"\nğŸ’¡ ë¨¼ì € LSTM ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”!")
        sys.exit(1)
    
    print("âœ… All files found! Starting LSTM localization...\n")
    main()
