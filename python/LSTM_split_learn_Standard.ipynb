{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68855827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485c3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cfd5074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„° í¬ê¸°:\n",
      "  data_lstm_test: 10348 rows, 28 UEs\n",
      "  data_lstm_5: 50478 rows, 28 UEs\n",
      "  data_lstm_3: 50478 rows, 28 UEs\n",
      "  data_lstm_1: 50478 rows, 28 UEs\n",
      "  data_lstm_6: 84448 rows, 28 UEs\n",
      "  data_lstm_4: 50478 rows, 28 UEs\n",
      "  data_lstm_0: 132392 rows, 28 UEs\n",
      "  data_lstm_2: 50478 rows, 28 UEs\n",
      "\n",
      "ğŸ¯ Test scenario: data_lstm_test (10348 rows)\n",
      "ğŸ¯ Train scenarios: ['data_lstm_5', 'data_lstm_3', 'data_lstm_1', 'data_lstm_6', 'data_lstm_4', 'data_lstm_0', 'data_lstm_2'] (ì´ 7ê°œ)\n",
      "\n",
      "ğŸ“‹ Feature ì»¬ëŸ¼: 7ê°œ\n",
      "ğŸ“‹ Target ì»¬ëŸ¼: 2ê°œ\n"
     ]
    }
   ],
   "source": [
    "# 1. ì‹œë‚˜ë¦¬ì˜¤ë³„ CSV íŒŒì¼ ë¡œë“œ ë° ë¶„ì„\n",
    "# ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ë“¤ ì°¾ê¸°\n",
    "scenario_dir = Path(\"data/scenarios\")  # ì‹œë‚˜ë¦¬ì˜¤ CSVë“¤ì´ ìˆëŠ” í´ë”\n",
    "scenario_files = list(scenario_dir.glob(\"*.csv\"))\n",
    "\n",
    "scenario_info = []\n",
    "for scenario_file in scenario_files:\n",
    "    df_temp = pd.read_csv(scenario_file)\n",
    "    scenario_info.append({\n",
    "        'file': scenario_file,\n",
    "        'name': scenario_file.stem,\n",
    "        'rows': len(df_temp),\n",
    "        'ues': df_temp['imsi'].nunique()\n",
    "    })\n",
    "\n",
    "print(\"ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„° í¬ê¸°:\")\n",
    "for info in scenario_info:\n",
    "    print(f\"  {info['name']}: {info['rows']} rows, {info['ues']} UEs\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì§ì ‘ ì§€ì •\n",
    "test_scenario_name = \"data_lstm_test\"  # ì›í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ìœ¼ë¡œ ë³€ê²½\n",
    "test_scenario = next((info for info in scenario_info if info['name'] == test_scenario_name), None)\n",
    "\n",
    "if test_scenario is None:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ '{test_scenario_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"   ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤: {[info['name'] for info in scenario_info]}\")\n",
    "    exit()\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ trainìœ¼ë¡œ\n",
    "train_scenarios = [info for info in scenario_info if info['name'] != test_scenario_name]\n",
    "\n",
    "print(f\"\\nğŸ¯ Test scenario: {test_scenario['name']} ({test_scenario['rows']} rows)\")\n",
    "print(f\"ğŸ¯ Train scenarios: {[s['name'] for s in train_scenarios]} (ì´ {len(train_scenarios)}ê°œ)\")\n",
    "\n",
    "# 2. í”¼ì²˜ ì»¬ëŸ¼ ì •ì˜ (imsi ì¸ì½”ë”© ì œê±°)\n",
    "feature_cols = [\n",
    "    \"relative_timestamp\",                           \n",
    "    \"serving_x\", \"serving_y\",                      \n",
    "    \"L3 serving SINR 3gpp_ma\",                    \n",
    "    \"L3 neigh SINR 3gpp 1 (convertedSinr)_ma\",   \n",
    "    \"L3 neigh SINR 3gpp 2 (convertedSinr)_ma\",   \n",
    "    \"L3 neigh SINR 3gpp 3 (convertedSinr)_ma\"    \n",
    "]\n",
    "target_cols = [\"UE_x\", \"UE_y\"]\n",
    "\n",
    "print(f\"\\nğŸ“‹ Feature ì»¬ëŸ¼: {len(feature_cols)}ê°œ\")\n",
    "print(f\"ğŸ“‹ Target ì»¬ëŸ¼: {len(target_cols)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d39a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 4. ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "class UESequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86398d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train sequences: data_lstm_5\n",
      "Processing train sequences: data_lstm_3\n",
      "Processing train sequences: data_lstm_1\n",
      "Processing train sequences: data_lstm_6\n",
      "Processing train sequences: data_lstm_4\n",
      "Processing train sequences: data_lstm_0\n",
      "Processing train sequences: data_lstm_2\n",
      "Processing test sequences: data_lstm_test\n",
      "Train ë°ì´í„°: X=(467270, 10, 7), y=(467270, 2)\n",
      "Test ë°ì´í„°: X=(10068, 10, 7), y=(10068, 2)\n"
     ]
    }
   ],
   "source": [
    "# 5. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì‹œí€€ìŠ¤ ìƒì„± (ê²½ê³„ ë„˜ì§€ ì•ŠìŒ)\n",
    "LOOKBACK = 10\n",
    "\n",
    "# Train ì‹œí€€ìŠ¤ ìƒì„±\n",
    "train_seq_X, train_seq_y, train_seq_imsi = [], [], []\n",
    "\n",
    "for train_info in train_scenarios:\n",
    "    print(f\"Processing train sequences: {train_info['name']}\")\n",
    "    df = pd.read_csv(train_info['file'])\n",
    "    df[\"imsi\"] = df[\"imsi\"].astype(\"category\").cat.codes\n",
    "    \n",
    "    for imsi_val, g in df.groupby(\"imsi\"):\n",
    "        g = g.sort_values(\"relative_timestamp\")\n",
    "        arr = g[feature_cols].to_numpy(dtype=\"float32\")\n",
    "        tgt = g[target_cols].to_numpy(dtype=\"float32\")\n",
    "        \n",
    "        # ì‹œë‚˜ë¦¬ì˜¤ ë‚´ì—ì„œë§Œ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "        for i in range(len(arr) - LOOKBACK):\n",
    "            train_seq_X.append(arr[i : i + LOOKBACK])\n",
    "            train_seq_y.append(tgt[i + LOOKBACK])\n",
    "            train_seq_imsi.append(f\"{train_info['name']}_{imsi_val}\")\n",
    "\n",
    "# Test ì‹œí€€ìŠ¤ ìƒì„±\n",
    "test_seq_X, test_seq_y, test_seq_imsi = [], [], []\n",
    "\n",
    "print(f\"Processing test sequences: {test_scenario['name']}\")\n",
    "df = pd.read_csv(test_scenario['file'])\n",
    "df[\"imsi\"] = df[\"imsi\"].astype(\"category\").cat.codes\n",
    "\n",
    "for imsi_val, g in df.groupby(\"imsi\"):\n",
    "    g = g.sort_values(\"relative_timestamp\")\n",
    "    arr = g[feature_cols].to_numpy(dtype=\"float32\")\n",
    "    tgt = g[target_cols].to_numpy(dtype=\"float32\")\n",
    "    \n",
    "    for i in range(len(arr) - LOOKBACK):\n",
    "        test_seq_X.append(arr[i : i + LOOKBACK])\n",
    "        test_seq_y.append(tgt[i + LOOKBACK])\n",
    "        test_seq_imsi.append(f\"{test_scenario['name']}_{imsi_val}\")\n",
    "\n",
    "# ë°°ì—´ ë³€í™˜\n",
    "X_train = np.array(train_seq_X)\n",
    "y_train = np.array(train_seq_y)\n",
    "imsi_train = np.array(train_seq_imsi)\n",
    "\n",
    "X_test = np.array(test_seq_X)\n",
    "y_test = np.array(test_seq_y)\n",
    "imsi_test = np.array(test_seq_imsi)\n",
    "\n",
    "print(f\"Train ë°ì´í„°: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test ë°ì´í„°: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a6c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 9. PyTorch LSTM ëª¨ë¸ ì •ì˜\n",
    "class UELocalizationLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=3, output_size=2):\n",
    "        super(UELocalizationLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 32)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì‹œì  ì¶œë ¥ ì‚¬ìš©\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Dense layers\n",
    "        out = F.relu(self.fc1(last_output))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c4d5755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Testing StandardScaler...\n",
      "  Training with StandardScaler...\n",
      "  Epoch [20/50], Loss: 0.015291\n",
      "  Epoch [40/50], Loss: 0.014625\n",
      "  Epoch [60/50], Loss: 0.014399\n",
      "  Epoch [80/50], Loss: 0.014253\n",
      "  Epoch [100/50], Loss: 0.014217\n",
      "  âœ… StandardScaler RMSE: 7.98m\n",
      "\n",
      "ğŸ† Best Scaler: Standard (RMSE: 7.98m)\n",
      "\n",
      "ğŸ“Š ì „ì²´ ê²°ê³¼:\n",
      "  Standard: 7.98m\n",
      "\n",
      "âœ… StandardScalerë¥¼ ìµœì¢… ëª¨ë¸ë¡œ ì„ íƒ\n"
     ]
    }
   ],
   "source": [
    "# 7-12. ìŠ¤ì¼€ì¼ëŸ¬ë³„ ë¹„êµ í…ŒìŠ¤íŠ¸\n",
    "scalers = {\n",
    "    'Standard': StandardScaler()\n",
    "}\n",
    "\n",
    "scaler_results = {}\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "for scaler_name, scaler_X in scalers.items():\n",
    "    print(f\"\\nğŸ§ª Testing {scaler_name}Scaler...\")\n",
    "    \n",
    "    # YëŠ” í•­ìƒ StandardScaler (ì¢Œí‘œëŠ” í‘œì¤€í™”ê°€ ì¢‹ìŒ)\n",
    "    scaler_Y = StandardScaler()\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\n",
    "    X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_Y.transform(y_test)\n",
    "    \n",
    "    # ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    train_dataset = UESequenceDataset(X_train_scaled, y_train_scaled)\n",
    "    test_dataset = UESequenceDataset(X_test_scaled, y_test_scaled)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = UELocalizationLSTM(input_size=n_features).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    \n",
    "    # í›ˆë ¨ í•¨ìˆ˜ (ë™ì¼)\n",
    "    def train_model_scaler(model, train_loader, criterion, optimizer, epochs=100):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            train_losses.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:  # ëœ ìì£¼ ì¶œë ¥\n",
    "                print(f'  Epoch [{epoch+1}/50], Loss: {avg_loss:.6f}')\n",
    "        \n",
    "        return train_losses\n",
    "\n",
    "    # ëª¨ë¸ í›ˆë ¨\n",
    "    print(f\"  Training with {scaler_name}Scaler...\")\n",
    "    train_losses = train_model_scaler(model, train_loader, criterion, optimizer, epochs=100)\n",
    "    \n",
    "    # í‰ê°€\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_targets.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.vstack(all_predictions)\n",
    "    targets = np.vstack(all_targets)\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§ ë˜ëŒë¦¬ê¸°\n",
    "    predictions_unscaled = scaler_Y.inverse_transform(predictions)\n",
    "    targets_unscaled = scaler_Y.inverse_transform(targets)\n",
    "    \n",
    "    # RMSE ê³„ì‚°\n",
    "    rmse = np.sqrt(np.mean((predictions_unscaled - targets_unscaled)**2))\n",
    "    \n",
    "    scaler_results[scaler_name] = {\n",
    "        'rmse': rmse,\n",
    "        'model': model,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_Y': scaler_Y,\n",
    "        'train_losses': train_losses\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ… {scaler_name}Scaler RMSE: {rmse:.2f}m\")\n",
    "\n",
    "# ìµœê³  ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ\n",
    "best_scaler_name = min(scaler_results.items(), key=lambda x: x[1]['rmse'])[0]\n",
    "best_result = scaler_results[best_scaler_name]\n",
    "\n",
    "print(f\"\\nğŸ† Best Scaler: {best_scaler_name} (RMSE: {best_result['rmse']:.2f}m)\")\n",
    "print(\"\\nğŸ“Š ì „ì²´ ê²°ê³¼:\")\n",
    "for name, result in scaler_results.items():\n",
    "    print(f\"  {name}: {result['rmse']:.2f}m\")\n",
    "\n",
    "# ìµœê³  ëª¨ë¸ë¡œ ì„¤ì •\n",
    "model = best_result['model']\n",
    "scX = best_result['scaler_X'] \n",
    "scY = best_result['scaler_Y']\n",
    "train_losses = best_result['train_losses']\n",
    "rmse = best_result['rmse']\n",
    "\n",
    "print(f\"\\nâœ… {best_scaler_name}Scalerë¥¼ ìµœì¢… ëª¨ë¸ë¡œ ì„ íƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54145da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PyTorch ëª¨ë¸ ì €ì¥ ì™„ë£Œ: pytorch_lstm_positioning.pth\n",
      "âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# %% 15. ëª¨ë¸ ì €ì¥\n",
    "# PyTorch ë°©ì‹ - h5py ì—†ì´!\n",
    "model_save_dict = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_size': n_features,\n",
    "        'hidden_size': 64,\n",
    "        'output_size': 2\n",
    "    },\n",
    "    'train_losses': train_losses,\n",
    "    'rmse': rmse\n",
    "}\n",
    "\n",
    "torch.save(model_save_dict, 'lstm_positioning.pth')\n",
    "print(\"âœ… PyTorch ëª¨ë¸ ì €ì¥ ì™„ë£Œ: pytorch_lstm_positioning.pth\")\n",
    "\n",
    "# 16. ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "joblib.dump(scX, \"lstm_x.pkl\")\n",
    "joblib.dump(scY, \"lstm_y.pkl\")\n",
    "print(\"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55432716",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 220\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# ì‹¤í–‰ (ê¸°ì¡´ ì½”ë“œ ë§ˆì§€ë§‰ì— ì¶”ê°€)\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m plot_comprehensive_analysis(\u001b[43mresults_df\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ì‹œê°í™” ì½”ë“œ (ë§¨ ë§ˆì§€ë§‰ì— ì¶”ê°€)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Circle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "def plot_ue_trajectories(results_df, max_ues=6, figsize=(15, 10)):\n",
    "    \"\"\"UEë³„ ê¶¤ì  ë¹„êµ (ì˜ˆì¸¡ vs ì‹¤ì œ)\"\"\"\n",
    "    \n",
    "    # ë°ì´í„°ê°€ ë§ì€ ìƒìœ„ UEë“¤ ì„ íƒ\n",
    "    ue_counts = results_df['imsi'].value_counts().head(max_ues)\n",
    "    selected_ues = ue_counts.index.tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ue_id in enumerate(selected_ues):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ue_data = results_df[results_df['imsi'] == ue_id].copy()\n",
    "        ue_data = ue_data.sort_index()  # ì‹œê°„ìˆœ ì •ë ¬\n",
    "        \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # ì‹¤ì œ ê¶¤ì ì€ ì„ ìœ¼ë¡œ ì—°ê²° (ì‹œê°„ ìˆœì„œëŒ€ë¡œ)\n",
    "        ax.scatter(ue_data['true_x'], ue_data['true_y'], \n",
    "                color='blue', alpha=0.6, s=20, zorder=3)\n",
    "\n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì ë§Œ í‘œì‹œ (ì„  ì—°ê²° ì•ˆ í•¨)\n",
    "        ax.scatter(ue_data['pred_x'], ue_data['pred_y'], \n",
    "                color='red', alpha=0.7, s=25, marker='s', \n",
    "                label='Predicted Points', zorder=4)\n",
    "        \n",
    "        # ì‹œì‘ì ê³¼ ëì  í‘œì‹œ\n",
    "        ax.scatter(ue_data['true_x'].iloc[0], ue_data['true_y'].iloc[0], \n",
    "                  color='green', s=100, marker='o', label='Start', zorder=5)\n",
    "        ax.scatter(ue_data['true_x'].iloc[-1], ue_data['true_y'].iloc[-1], \n",
    "                  color='black', s=100, marker='X', label='End', zorder=5)\n",
    "        \n",
    "        # UEë³„ RMSE ê³„ì‚°\n",
    "        ue_rmse = np.sqrt(np.mean((ue_data['pred_x'] - ue_data['true_x'])**2 + \n",
    "                                 (ue_data['pred_y'] - ue_data['true_y'])**2))\n",
    "        \n",
    "        ax.set_title(f'UE {ue_id} Trajectory\\n({len(ue_data)} points, RMSE: {ue_rmse:.1f}m)')\n",
    "        ax.set_xlabel('X Coordinate (m)')\n",
    "        ax.set_ylabel('Y Coordinate (m)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axis('equal')\n",
    "    \n",
    "    # ë¹ˆ subplot ì œê±°\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ue_trajectories.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_error_analysis(results_df, figsize=(15, 12)):\n",
    "    \"\"\"ì˜¤ì°¨ ë¶„ì„ ë‹¤ì–‘í•œ ê´€ì \"\"\"\n",
    "    \n",
    "    # ì˜¤ì°¨ ê³„ì‚°\n",
    "    results_df['error_x'] = results_df['pred_x'] - results_df['true_x']\n",
    "    results_df['error_y'] = results_df['pred_y'] - results_df['true_y']\n",
    "    results_df['error_distance'] = np.sqrt(results_df['error_x']**2 + results_df['error_y']**2)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "    \n",
    "    # 1. UEë³„ RMSE íˆìŠ¤í† ê·¸ë¨\n",
    "    ue_rmse = results_df.groupby('imsi').apply(\n",
    "        lambda x: np.sqrt(np.mean(x['error_distance']**2))\n",
    "    ).reset_index(name='rmse')\n",
    "    \n",
    "    axes[0,0].hist(ue_rmse['rmse'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].axvline(ue_rmse['rmse'].mean(), color='red', linestyle='--', \n",
    "                     label=f'Mean: {ue_rmse[\"rmse\"].mean():.1f}m')\n",
    "    axes[0,0].set_title('UEë³„ RMSE ë¶„í¬')\n",
    "    axes[0,0].set_xlabel('RMSE (m)')\n",
    "    axes[0,0].set_ylabel('UE ê°œìˆ˜')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ì˜¤ì°¨ ê±°ë¦¬ íˆìŠ¤í† ê·¸ë¨\n",
    "    axes[0,1].hist(results_df['error_distance'], bins=50, alpha=0.7, \n",
    "                   color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].axvline(results_df['error_distance'].mean(), color='red', linestyle='--',\n",
    "                     label=f'Mean: {results_df[\"error_distance\"].mean():.1f}m')\n",
    "    axes[0,1].set_title('ì˜ˆì¸¡ ì˜¤ì°¨ ê±°ë¦¬ ë¶„í¬')\n",
    "    axes[0,1].set_xlabel('Error Distance (m)')\n",
    "    axes[0,1].set_ylabel('ìƒ˜í”Œ ê°œìˆ˜')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. X, Y ì˜¤ì°¨ ì‚°ì ë„\n",
    "    axes[0,2].scatter(results_df['error_x'], results_df['error_y'], \n",
    "                     alpha=0.5, s=10, color='purple')\n",
    "    axes[0,2].axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0,2].axvline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0,2].set_title('X-Y ì˜¤ì°¨ ë¶„í¬')\n",
    "    axes[0,2].set_xlabel('X Error (m)')\n",
    "    axes[0,2].set_ylabel('Y Error (m)')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    axes[0,2].axis('equal')\n",
    "    \n",
    "    # 4. UEë³„ í‰ê·  ì˜¤ì°¨ (ìƒìœ„ 10ê°œ)\n",
    "    top_ues = results_df.groupby('imsi')['error_distance'].mean().nlargest(10)\n",
    "    axes[1,0].barh(range(len(top_ues)), top_ues.values, color='orange', alpha=0.7)\n",
    "    axes[1,0].set_yticks(range(len(top_ues)))\n",
    "    axes[1,0].set_yticklabels([f'UE {ue}' for ue in top_ues.index])\n",
    "    axes[1,0].set_title('ì˜¤ì°¨ê°€ í° ìƒìœ„ 10ê°œ UE')\n",
    "    axes[1,0].set_xlabel('Average Error Distance (m)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. ìƒ˜í”Œ ìˆ˜ vs ì˜¤ì°¨ ê´€ê³„\n",
    "    ue_stats = results_df.groupby('imsi').agg({\n",
    "        'error_distance': 'mean',\n",
    "        'imsi': 'count'\n",
    "    }).rename(columns={'imsi': 'sample_count'})\n",
    "    \n",
    "    axes[1,1].scatter(ue_stats['sample_count'], ue_stats['error_distance'], \n",
    "                     alpha=0.7, s=50, color='green')\n",
    "    axes[1,1].set_title('ìƒ˜í”Œ ìˆ˜ vs í‰ê·  ì˜¤ì°¨')\n",
    "    axes[1,1].set_xlabel('Sample Count per UE')\n",
    "    axes[1,1].set_ylabel('Average Error Distance (m)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. ëˆ„ì  ì˜¤ì°¨ ë¶„í¬ (CDF)\n",
    "    sorted_errors = np.sort(results_df['error_distance'])\n",
    "    p = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    axes[1,2].plot(sorted_errors, p, linewidth=2, color='navy')\n",
    "    axes[1,2].axvline(np.percentile(sorted_errors, 50), color='red', linestyle='--', \n",
    "                     label=f'50th: {np.percentile(sorted_errors, 50):.1f}m')\n",
    "    axes[1,2].axvline(np.percentile(sorted_errors, 90), color='orange', linestyle='--',\n",
    "                     label=f'90th: {np.percentile(sorted_errors, 90):.1f}m')\n",
    "    axes[1,2].set_title('ì˜¤ì°¨ ëˆ„ì  ë¶„í¬ (CDF)')\n",
    "    axes[1,2].set_xlabel('Error Distance (m)')\n",
    "    axes[1,2].set_ylabel('Cumulative Probability')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    print(f\"ğŸ“Š ì˜¤ì°¨ í†µê³„:\")\n",
    "    print(f\"  í‰ê·  ì˜¤ì°¨: {results_df['error_distance'].mean():.2f} m\")\n",
    "    print(f\"  ì¤‘ìœ„ìˆ˜ ì˜¤ì°¨: {results_df['error_distance'].median():.2f} m\")\n",
    "    print(f\"  90th percentile: {np.percentile(results_df['error_distance'], 90):.2f} m\")\n",
    "    print(f\"  ìµœëŒ€ ì˜¤ì°¨: {results_df['error_distance'].max():.2f} m\")\n",
    "\n",
    "# %%\n",
    "def plot_prediction_scatter(results_df, figsize=(12, 5)):\n",
    "    \"\"\"ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # X ì¢Œí‘œ ë¹„êµ\n",
    "    axes[0].scatter(results_df['true_x'], results_df['pred_x'], \n",
    "                   alpha=0.6, s=20, color='blue')\n",
    "    \n",
    "    # ì™„ë²½í•œ ì˜ˆì¸¡ì„  (y=x)\n",
    "    min_x, max_x = results_df['true_x'].min(), results_df['true_x'].max()\n",
    "    axes[0].plot([min_x, max_x], [min_x, max_x], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    axes[0].set_xlabel('True X (m)')\n",
    "    axes[0].set_ylabel('Predicted X (m)')\n",
    "    axes[0].set_title('X Coordinate Prediction')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axis('equal')\n",
    "    \n",
    "    # Y ì¢Œí‘œ ë¹„êµ\n",
    "    axes[1].scatter(results_df['true_y'], results_df['pred_y'], \n",
    "                   alpha=0.6, s=20, color='green')\n",
    "    \n",
    "    min_y, max_y = results_df['true_y'].min(), results_df['true_y'].max()\n",
    "    axes[1].plot([min_y, max_y], [min_y, max_y], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    axes[1].set_xlabel('True Y (m)')\n",
    "    axes[1].set_ylabel('Predicted Y (m)')\n",
    "    axes[1].set_title('Y Coordinate Prediction')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_scatter.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_comprehensive_analysis(results_df):\n",
    "    \"\"\"ì¢…í•© ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸ¨ ì‹œê°í™” ì‹œì‘...\")\n",
    "    \n",
    "    # 1. UEë³„ ê¶¤ì \n",
    "    print(\"1. UEë³„ ê¶¤ì  ê·¸ë˜í”„...\")\n",
    "    plot_ue_trajectories(results_df, max_ues=6)\n",
    "    \n",
    "    # 2. ì˜¤ì°¨ ë¶„ì„\n",
    "    print(\"2. ì˜¤ì°¨ ë¶„ì„ ê·¸ë˜í”„...\")\n",
    "    plot_error_analysis(results_df)\n",
    "    \n",
    "    # 3. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„\n",
    "    print(\"3. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„...\")\n",
    "    plot_prediction_scatter(results_df)\n",
    "    \n",
    "    print(\"âœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ!\")\n",
    "\n",
    "# %%\n",
    "# ì‹¤í–‰ (ê¸°ì¡´ ì½”ë“œ ë§ˆì§€ë§‰ì— ì¶”ê°€)\n",
    "plot_comprehensive_analysis(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
