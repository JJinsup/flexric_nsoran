{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68855827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485c3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cfd5074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ÏãúÎÇòÎ¶¨Ïò§Î≥Ñ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞:\n",
      "  data_lstm_test: 10348 rows, 28 UEs\n",
      "  data_lstm_5: 50478 rows, 28 UEs\n",
      "  data_lstm_3: 50478 rows, 28 UEs\n",
      "  data_lstm_1: 50478 rows, 28 UEs\n",
      "  data_lstm_6: 84448 rows, 28 UEs\n",
      "  data_lstm_4: 50478 rows, 28 UEs\n",
      "  data_lstm_0: 132392 rows, 28 UEs\n",
      "  data_lstm_2: 50478 rows, 28 UEs\n",
      "\n",
      "üéØ Test scenario: data_lstm_test (10348 rows)\n",
      "üéØ Train scenarios: ['data_lstm_5', 'data_lstm_3', 'data_lstm_1', 'data_lstm_6', 'data_lstm_4', 'data_lstm_0', 'data_lstm_2'] (Ï¥ù 7Í∞ú)\n",
      "\n",
      "üìã Feature Ïª¨Îüº: 7Í∞ú\n",
      "üìã Target Ïª¨Îüº: 2Í∞ú\n"
     ]
    }
   ],
   "source": [
    "# 1. ÏãúÎÇòÎ¶¨Ïò§Î≥Ñ CSV ÌååÏùº Î°úÎìú Î∞è Î∂ÑÏÑù\n",
    "# ÏãúÎÇòÎ¶¨Ïò§ ÌååÏùºÎì§ Ï∞æÍ∏∞\n",
    "scenario_dir = Path(\"data/scenarios\")  # ÏãúÎÇòÎ¶¨Ïò§ CSVÎì§Ïù¥ ÏûàÎäî Ìè¥Îçî\n",
    "scenario_files = list(scenario_dir.glob(\"*.csv\"))\n",
    "\n",
    "scenario_info = []\n",
    "for scenario_file in scenario_files:\n",
    "    df_temp = pd.read_csv(scenario_file)\n",
    "    scenario_info.append({\n",
    "        'file': scenario_file,\n",
    "        'name': scenario_file.stem,\n",
    "        'rows': len(df_temp),\n",
    "        'ues': df_temp['imsi'].nunique()\n",
    "    })\n",
    "\n",
    "print(\"üìä ÏãúÎÇòÎ¶¨Ïò§Î≥Ñ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞:\")\n",
    "for info in scenario_info:\n",
    "    print(f\"  {info['name']}: {info['rows']} rows, {info['ues']} UEs\")\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ ÏãúÎÇòÎ¶¨Ïò§ ÏßÅÏ†ë ÏßÄÏ†ï\n",
    "test_scenario_name = \"data_lstm_test\"  # ÏõêÌïòÎäî ÏãúÎÇòÎ¶¨Ïò§ Ïù¥Î¶ÑÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "test_scenario = next((info for info in scenario_info if info['name'] == test_scenario_name), None)\n",
    "\n",
    "if test_scenario is None:\n",
    "    print(f\"‚ùå ÌÖåÏä§Ìä∏ ÏãúÎÇòÎ¶¨Ïò§ '{test_scenario_name}'Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§!\")\n",
    "    print(f\"   ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏãúÎÇòÎ¶¨Ïò§: {[info['name'] for info in scenario_info]}\")\n",
    "    exit()\n",
    "\n",
    "# ÎÇòÎ®∏ÏßÄ ÏãúÎÇòÎ¶¨Ïò§Îì§ÏùÑ trainÏúºÎ°ú\n",
    "train_scenarios = [info for info in scenario_info if info['name'] != test_scenario_name]\n",
    "\n",
    "print(f\"\\nüéØ Test scenario: {test_scenario['name']} ({test_scenario['rows']} rows)\")\n",
    "print(f\"üéØ Train scenarios: {[s['name'] for s in train_scenarios]} (Ï¥ù {len(train_scenarios)}Í∞ú)\")\n",
    "\n",
    "# 2. ÌîºÏ≤ò Ïª¨Îüº Ï†ïÏùò (imsi Ïù∏ÏΩîÎî© Ï†úÍ±∞)\n",
    "feature_cols = [\n",
    "    \"relative_timestamp\",                           \n",
    "    \"serving_x\", \"serving_y\",                      \n",
    "    \"L3 serving SINR 3gpp_ma\",                    \n",
    "    \"L3 neigh SINR 3gpp 1 (convertedSinr)_ma\",   \n",
    "    \"L3 neigh SINR 3gpp 2 (convertedSinr)_ma\",   \n",
    "    \"L3 neigh SINR 3gpp 3 (convertedSinr)_ma\"    \n",
    "]\n",
    "target_cols = [\"UE_x\", \"UE_y\"]\n",
    "\n",
    "print(f\"\\nüìã Feature Ïª¨Îüº: {len(feature_cols)}Í∞ú\")\n",
    "print(f\"üìã Target Ïª¨Îüº: {len(target_cols)}Í∞ú\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d39a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 4. ÏãúÌÄÄÏä§ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§\n",
    "class UESequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86398d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train sequences: data_lstm_5\n",
      "Processing train sequences: data_lstm_3\n",
      "Processing train sequences: data_lstm_1\n",
      "Processing train sequences: data_lstm_6\n",
      "Processing train sequences: data_lstm_4\n",
      "Processing train sequences: data_lstm_0\n",
      "Processing train sequences: data_lstm_2\n",
      "Processing test sequences: data_lstm_test\n",
      "Train Îç∞Ïù¥ÌÑ∞: X=(467270, 10, 7), y=(467270, 2)\n",
      "Test Îç∞Ïù¥ÌÑ∞: X=(10068, 10, 7), y=(10068, 2)\n"
     ]
    }
   ],
   "source": [
    "# 5. ÏãúÎÇòÎ¶¨Ïò§Î≥Ñ ÏãúÌÄÄÏä§ ÏÉùÏÑ± (Í≤ΩÍ≥Ñ ÎÑòÏßÄ ÏïäÏùå)\n",
    "LOOKBACK = 10\n",
    "\n",
    "# Train ÏãúÌÄÄÏä§ ÏÉùÏÑ±\n",
    "train_seq_X, train_seq_y, train_seq_imsi = [], [], []\n",
    "\n",
    "for train_info in train_scenarios:\n",
    "    print(f\"Processing train sequences: {train_info['name']}\")\n",
    "    df = pd.read_csv(train_info['file'])\n",
    "    df[\"imsi\"] = df[\"imsi\"].astype(\"category\").cat.codes\n",
    "    \n",
    "    for imsi_val, g in df.groupby(\"imsi\"):\n",
    "        g = g.sort_values(\"relative_timestamp\")\n",
    "        arr = g[feature_cols].to_numpy(dtype=\"float32\")\n",
    "        tgt = g[target_cols].to_numpy(dtype=\"float32\")\n",
    "        \n",
    "        # ÏãúÎÇòÎ¶¨Ïò§ ÎÇ¥ÏóêÏÑúÎßå ÏãúÌÄÄÏä§ ÏÉùÏÑ±\n",
    "        for i in range(len(arr) - LOOKBACK):\n",
    "            train_seq_X.append(arr[i : i + LOOKBACK])\n",
    "            train_seq_y.append(tgt[i + LOOKBACK])\n",
    "            train_seq_imsi.append(f\"{train_info['name']}_{imsi_val}\")\n",
    "\n",
    "# Test ÏãúÌÄÄÏä§ ÏÉùÏÑ±\n",
    "test_seq_X, test_seq_y, test_seq_imsi = [], [], []\n",
    "\n",
    "print(f\"Processing test sequences: {test_scenario['name']}\")\n",
    "df = pd.read_csv(test_scenario['file'])\n",
    "df[\"imsi\"] = df[\"imsi\"].astype(\"category\").cat.codes\n",
    "\n",
    "for imsi_val, g in df.groupby(\"imsi\"):\n",
    "    g = g.sort_values(\"relative_timestamp\")\n",
    "    arr = g[feature_cols].to_numpy(dtype=\"float32\")\n",
    "    tgt = g[target_cols].to_numpy(dtype=\"float32\")\n",
    "    \n",
    "    for i in range(len(arr) - LOOKBACK):\n",
    "        test_seq_X.append(arr[i : i + LOOKBACK])\n",
    "        test_seq_y.append(tgt[i + LOOKBACK])\n",
    "        test_seq_imsi.append(f\"{test_scenario['name']}_{imsi_val}\")\n",
    "\n",
    "# Î∞∞Ïó¥ Î≥ÄÌôò\n",
    "X_train = np.array(train_seq_X)\n",
    "y_train = np.array(train_seq_y)\n",
    "imsi_train = np.array(train_seq_imsi)\n",
    "\n",
    "X_test = np.array(test_seq_X)\n",
    "y_test = np.array(test_seq_y)\n",
    "imsi_test = np.array(test_seq_imsi)\n",
    "\n",
    "print(f\"Train Îç∞Ïù¥ÌÑ∞: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test Îç∞Ïù¥ÌÑ∞: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a6c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 9. PyTorch LSTM Î™®Îç∏ Ï†ïÏùò\n",
    "class UELocalizationLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=3, output_size=2):\n",
    "        super(UELocalizationLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 32)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # ÎßàÏßÄÎßâ ÏãúÏ†ê Ï∂úÎ†• ÏÇ¨Ïö©\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Dense layers\n",
    "        out = F.relu(self.fc1(last_output))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c4d5755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing StandardScaler...\n",
      "  Training with StandardScaler...\n",
      "  Epoch [20/50], Loss: 0.015291\n",
      "  Epoch [40/50], Loss: 0.014625\n",
      "  Epoch [60/50], Loss: 0.014399\n",
      "  Epoch [80/50], Loss: 0.014253\n",
      "  Epoch [100/50], Loss: 0.014217\n",
      "  ‚úÖ StandardScaler RMSE: 7.98m\n",
      "\n",
      "üèÜ Best Scaler: Standard (RMSE: 7.98m)\n",
      "\n",
      "üìä Ï†ÑÏ≤¥ Í≤∞Í≥º:\n",
      "  Standard: 7.98m\n",
      "\n",
      "‚úÖ StandardScalerÎ•º ÏµúÏ¢Ö Î™®Îç∏Î°ú ÏÑ†ÌÉù\n"
     ]
    }
   ],
   "source": [
    "# 7-12. Ïä§ÏºÄÏùºÎü¨Î≥Ñ ÎπÑÍµê ÌÖåÏä§Ìä∏\n",
    "scalers = {\n",
    "    'Standard': StandardScaler()\n",
    "}\n",
    "\n",
    "scaler_results = {}\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "for scaler_name, scaler_X in scalers.items():\n",
    "    print(f\"\\nüß™ Testing {scaler_name}Scaler...\")\n",
    "    \n",
    "    # YÎäî Ìï≠ÏÉÅ StandardScaler (Ï¢åÌëúÎäî ÌëúÏ§ÄÌôîÍ∞Ä Ï¢ãÏùå)\n",
    "    scaler_Y = StandardScaler()\n",
    "    \n",
    "    # Ïä§ÏºÄÏùºÎßÅ\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\n",
    "    X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n",
    "    y_train_scaled = scaler_Y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_Y.transform(y_test)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±\n",
    "    train_dataset = UESequenceDataset(X_train_scaled, y_train_scaled)\n",
    "    test_dataset = UESequenceDataset(X_test_scaled, y_test_scaled)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "    model = UELocalizationLSTM(input_size=n_features).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    \n",
    "    # ÌõàÎ†® Ìï®Ïàò (ÎèôÏùº)\n",
    "    def train_model_scaler(model, train_loader, criterion, optimizer, epochs=100):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            train_losses.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:  # Îçú ÏûêÏ£º Ï∂úÎ†•\n",
    "                print(f'  Epoch [{epoch+1}/50], Loss: {avg_loss:.6f}')\n",
    "        \n",
    "        return train_losses\n",
    "\n",
    "    # Î™®Îç∏ ÌõàÎ†®\n",
    "    print(f\"  Training with {scaler_name}Scaler...\")\n",
    "    train_losses = train_model_scaler(model, train_loader, criterion, optimizer, epochs=100)\n",
    "    \n",
    "    # ÌèâÍ∞Ä\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_targets.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.vstack(all_predictions)\n",
    "    targets = np.vstack(all_targets)\n",
    "    \n",
    "    # Ïä§ÏºÄÏùºÎßÅ ÎêòÎèåÎ¶¨Í∏∞\n",
    "    predictions_unscaled = scaler_Y.inverse_transform(predictions)\n",
    "    targets_unscaled = scaler_Y.inverse_transform(targets)\n",
    "    \n",
    "    # RMSE Í≥ÑÏÇ∞\n",
    "    rmse = np.sqrt(np.mean((predictions_unscaled - targets_unscaled)**2))\n",
    "    \n",
    "    scaler_results[scaler_name] = {\n",
    "        'rmse': rmse,\n",
    "        'model': model,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_Y': scaler_Y,\n",
    "        'train_losses': train_losses\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úÖ {scaler_name}Scaler RMSE: {rmse:.2f}m\")\n",
    "\n",
    "# ÏµúÍ≥† Ïä§ÏºÄÏùºÎü¨ ÏÑ†ÌÉù\n",
    "best_scaler_name = min(scaler_results.items(), key=lambda x: x[1]['rmse'])[0]\n",
    "best_result = scaler_results[best_scaler_name]\n",
    "\n",
    "print(f\"\\nüèÜ Best Scaler: {best_scaler_name} (RMSE: {best_result['rmse']:.2f}m)\")\n",
    "print(\"\\nüìä Ï†ÑÏ≤¥ Í≤∞Í≥º:\")\n",
    "for name, result in scaler_results.items():\n",
    "    print(f\"  {name}: {result['rmse']:.2f}m\")\n",
    "\n",
    "# ÏµúÍ≥† Î™®Îç∏Î°ú ÏÑ§Ï†ï\n",
    "model = best_result['model']\n",
    "scX = best_result['scaler_X'] \n",
    "scY = best_result['scaler_Y']\n",
    "train_losses = best_result['train_losses']\n",
    "rmse = best_result['rmse']\n",
    "\n",
    "print(f\"\\n‚úÖ {best_scaler_name}ScalerÎ•º ÏµúÏ¢Ö Î™®Îç∏Î°ú ÏÑ†ÌÉù\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54145da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: pytorch_lstm_positioning.pth\n",
      "‚úÖ Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû• ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "# %% 15. Î™®Îç∏ Ï†ÄÏû•\n",
    "# PyTorch Î∞©Ïãù - h5py ÏóÜÏù¥!\n",
    "model_save_dict = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_size': n_features,\n",
    "        'hidden_size': 64,\n",
    "        'output_size': 2\n",
    "    },\n",
    "    'train_losses': train_losses,\n",
    "    'rmse': rmse\n",
    "}\n",
    "\n",
    "torch.save(model_save_dict, 'lstm_positioning.pth')\n",
    "print(\"‚úÖ PyTorch Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: pytorch_lstm_positioning.pth\")\n",
    "\n",
    "# 16. Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû•\n",
    "joblib.dump(scX, \"lstm_x.pkl\")\n",
    "joblib.dump(scY, \"lstm_y.pkl\")\n",
    "print(\"‚úÖ Ïä§ÏºÄÏùºÎü¨ Ï†ÄÏû• ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55432716",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 220\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Î™®Îì† ÏãúÍ∞ÅÌôî ÏôÑÎ£å!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Ïã§Ìñâ (Í∏∞Ï°¥ ÏΩîÎìú ÎßàÏßÄÎßâÏóê Ï∂îÍ∞Ä)\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m plot_comprehensive_analysis(\u001b[43mresults_df\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ÏãúÍ∞ÅÌôî ÏΩîÎìú (Îß® ÎßàÏßÄÎßâÏóê Ï∂îÍ∞Ä)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Circle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "def plot_ue_trajectories(results_df, max_ues=6, figsize=(15, 10)):\n",
    "    \"\"\"UEÎ≥Ñ Í∂§Ï†Å ÎπÑÍµê (ÏòàÏ∏° vs Ïã§Ï†ú)\"\"\"\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞Í∞Ä ÎßéÏùÄ ÏÉÅÏúÑ UEÎì§ ÏÑ†ÌÉù\n",
    "    ue_counts = results_df['imsi'].value_counts().head(max_ues)\n",
    "    selected_ues = ue_counts.index.tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ue_id in enumerate(selected_ues):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ue_data = results_df[results_df['imsi'] == ue_id].copy()\n",
    "        ue_data = ue_data.sort_index()  # ÏãúÍ∞ÑÏàú Ï†ïÎ†¨\n",
    "        \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Ïã§Ï†ú Í∂§Ï†ÅÏùÄ ÏÑ†ÏúºÎ°ú Ïó∞Í≤∞ (ÏãúÍ∞Ñ ÏàúÏÑúÎåÄÎ°ú)\n",
    "        ax.scatter(ue_data['true_x'], ue_data['true_y'], \n",
    "                color='blue', alpha=0.6, s=20, zorder=3)\n",
    "\n",
    "        # ÏòàÏ∏° Í≤∞Í≥ºÎäî Ï†êÎßå ÌëúÏãú (ÏÑ† Ïó∞Í≤∞ Ïïà Ìï®)\n",
    "        ax.scatter(ue_data['pred_x'], ue_data['pred_y'], \n",
    "                color='red', alpha=0.7, s=25, marker='s', \n",
    "                label='Predicted Points', zorder=4)\n",
    "        \n",
    "        # ÏãúÏûëÏ†êÍ≥º ÎÅùÏ†ê ÌëúÏãú\n",
    "        ax.scatter(ue_data['true_x'].iloc[0], ue_data['true_y'].iloc[0], \n",
    "                  color='green', s=100, marker='o', label='Start', zorder=5)\n",
    "        ax.scatter(ue_data['true_x'].iloc[-1], ue_data['true_y'].iloc[-1], \n",
    "                  color='black', s=100, marker='X', label='End', zorder=5)\n",
    "        \n",
    "        # UEÎ≥Ñ RMSE Í≥ÑÏÇ∞\n",
    "        ue_rmse = np.sqrt(np.mean((ue_data['pred_x'] - ue_data['true_x'])**2 + \n",
    "                                 (ue_data['pred_y'] - ue_data['true_y'])**2))\n",
    "        \n",
    "        ax.set_title(f'UE {ue_id} Trajectory\\n({len(ue_data)} points, RMSE: {ue_rmse:.1f}m)')\n",
    "        ax.set_xlabel('X Coordinate (m)')\n",
    "        ax.set_ylabel('Y Coordinate (m)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axis('equal')\n",
    "    \n",
    "    # Îπà subplot Ï†úÍ±∞\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ue_trajectories.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_error_analysis(results_df, figsize=(15, 12)):\n",
    "    \"\"\"Ïò§Ï∞® Î∂ÑÏÑù Îã§ÏñëÌïú Í¥ÄÏ†ê\"\"\"\n",
    "    \n",
    "    # Ïò§Ï∞® Í≥ÑÏÇ∞\n",
    "    results_df['error_x'] = results_df['pred_x'] - results_df['true_x']\n",
    "    results_df['error_y'] = results_df['pred_y'] - results_df['true_y']\n",
    "    results_df['error_distance'] = np.sqrt(results_df['error_x']**2 + results_df['error_y']**2)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "    \n",
    "    # 1. UEÎ≥Ñ RMSE ÌûàÏä§ÌÜ†Í∑∏Îû®\n",
    "    ue_rmse = results_df.groupby('imsi').apply(\n",
    "        lambda x: np.sqrt(np.mean(x['error_distance']**2))\n",
    "    ).reset_index(name='rmse')\n",
    "    \n",
    "    axes[0,0].hist(ue_rmse['rmse'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].axvline(ue_rmse['rmse'].mean(), color='red', linestyle='--', \n",
    "                     label=f'Mean: {ue_rmse[\"rmse\"].mean():.1f}m')\n",
    "    axes[0,0].set_title('UEÎ≥Ñ RMSE Î∂ÑÌè¨')\n",
    "    axes[0,0].set_xlabel('RMSE (m)')\n",
    "    axes[0,0].set_ylabel('UE Í∞úÏàò')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Ïò§Ï∞® Í±∞Î¶¨ ÌûàÏä§ÌÜ†Í∑∏Îû®\n",
    "    axes[0,1].hist(results_df['error_distance'], bins=50, alpha=0.7, \n",
    "                   color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].axvline(results_df['error_distance'].mean(), color='red', linestyle='--',\n",
    "                     label=f'Mean: {results_df[\"error_distance\"].mean():.1f}m')\n",
    "    axes[0,1].set_title('ÏòàÏ∏° Ïò§Ï∞® Í±∞Î¶¨ Î∂ÑÌè¨')\n",
    "    axes[0,1].set_xlabel('Error Distance (m)')\n",
    "    axes[0,1].set_ylabel('ÏÉòÌîå Í∞úÏàò')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. X, Y Ïò§Ï∞® ÏÇ∞Ï†êÎèÑ\n",
    "    axes[0,2].scatter(results_df['error_x'], results_df['error_y'], \n",
    "                     alpha=0.5, s=10, color='purple')\n",
    "    axes[0,2].axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0,2].axvline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0,2].set_title('X-Y Ïò§Ï∞® Î∂ÑÌè¨')\n",
    "    axes[0,2].set_xlabel('X Error (m)')\n",
    "    axes[0,2].set_ylabel('Y Error (m)')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    axes[0,2].axis('equal')\n",
    "    \n",
    "    # 4. UEÎ≥Ñ ÌèâÍ∑† Ïò§Ï∞® (ÏÉÅÏúÑ 10Í∞ú)\n",
    "    top_ues = results_df.groupby('imsi')['error_distance'].mean().nlargest(10)\n",
    "    axes[1,0].barh(range(len(top_ues)), top_ues.values, color='orange', alpha=0.7)\n",
    "    axes[1,0].set_yticks(range(len(top_ues)))\n",
    "    axes[1,0].set_yticklabels([f'UE {ue}' for ue in top_ues.index])\n",
    "    axes[1,0].set_title('Ïò§Ï∞®Í∞Ä ÌÅ∞ ÏÉÅÏúÑ 10Í∞ú UE')\n",
    "    axes[1,0].set_xlabel('Average Error Distance (m)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. ÏÉòÌîå Ïàò vs Ïò§Ï∞® Í¥ÄÍ≥Ñ\n",
    "    ue_stats = results_df.groupby('imsi').agg({\n",
    "        'error_distance': 'mean',\n",
    "        'imsi': 'count'\n",
    "    }).rename(columns={'imsi': 'sample_count'})\n",
    "    \n",
    "    axes[1,1].scatter(ue_stats['sample_count'], ue_stats['error_distance'], \n",
    "                     alpha=0.7, s=50, color='green')\n",
    "    axes[1,1].set_title('ÏÉòÌîå Ïàò vs ÌèâÍ∑† Ïò§Ï∞®')\n",
    "    axes[1,1].set_xlabel('Sample Count per UE')\n",
    "    axes[1,1].set_ylabel('Average Error Distance (m)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. ÎàÑÏ†Å Ïò§Ï∞® Î∂ÑÌè¨ (CDF)\n",
    "    sorted_errors = np.sort(results_df['error_distance'])\n",
    "    p = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    axes[1,2].plot(sorted_errors, p, linewidth=2, color='navy')\n",
    "    axes[1,2].axvline(np.percentile(sorted_errors, 50), color='red', linestyle='--', \n",
    "                     label=f'50th: {np.percentile(sorted_errors, 50):.1f}m')\n",
    "    axes[1,2].axvline(np.percentile(sorted_errors, 90), color='orange', linestyle='--',\n",
    "                     label=f'90th: {np.percentile(sorted_errors, 90):.1f}m')\n",
    "    axes[1,2].set_title('Ïò§Ï∞® ÎàÑÏ†Å Î∂ÑÌè¨ (CDF)')\n",
    "    axes[1,2].set_xlabel('Error Distance (m)')\n",
    "    axes[1,2].set_ylabel('Cumulative Probability')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "    print(f\"üìä Ïò§Ï∞® ÌÜµÍ≥Ñ:\")\n",
    "    print(f\"  ÌèâÍ∑† Ïò§Ï∞®: {results_df['error_distance'].mean():.2f} m\")\n",
    "    print(f\"  Ï§ëÏúÑÏàò Ïò§Ï∞®: {results_df['error_distance'].median():.2f} m\")\n",
    "    print(f\"  90th percentile: {np.percentile(results_df['error_distance'], 90):.2f} m\")\n",
    "    print(f\"  ÏµúÎåÄ Ïò§Ï∞®: {results_df['error_distance'].max():.2f} m\")\n",
    "\n",
    "# %%\n",
    "def plot_prediction_scatter(results_df, figsize=(12, 5)):\n",
    "    \"\"\"ÏòàÏ∏° vs Ïã§Ï†ú ÏÇ∞Ï†êÎèÑ\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # X Ï¢åÌëú ÎπÑÍµê\n",
    "    axes[0].scatter(results_df['true_x'], results_df['pred_x'], \n",
    "                   alpha=0.6, s=20, color='blue')\n",
    "    \n",
    "    # ÏôÑÎ≤ΩÌïú ÏòàÏ∏°ÏÑ† (y=x)\n",
    "    min_x, max_x = results_df['true_x'].min(), results_df['true_x'].max()\n",
    "    axes[0].plot([min_x, max_x], [min_x, max_x], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    axes[0].set_xlabel('True X (m)')\n",
    "    axes[0].set_ylabel('Predicted X (m)')\n",
    "    axes[0].set_title('X Coordinate Prediction')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axis('equal')\n",
    "    \n",
    "    # Y Ï¢åÌëú ÎπÑÍµê\n",
    "    axes[1].scatter(results_df['true_y'], results_df['pred_y'], \n",
    "                   alpha=0.6, s=20, color='green')\n",
    "    \n",
    "    min_y, max_y = results_df['true_y'].min(), results_df['true_y'].max()\n",
    "    axes[1].plot([min_y, max_y], [min_y, max_y], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    axes[1].set_xlabel('True Y (m)')\n",
    "    axes[1].set_ylabel('Predicted Y (m)')\n",
    "    axes[1].set_title('Y Coordinate Prediction')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_scatter.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "def plot_comprehensive_analysis(results_df):\n",
    "    \"\"\"Ï¢ÖÌï© Î∂ÑÏÑù Ïã§Ìñâ\"\"\"\n",
    "    print(\"üé® ÏãúÍ∞ÅÌôî ÏãúÏûë...\")\n",
    "    \n",
    "    # 1. UEÎ≥Ñ Í∂§Ï†Å\n",
    "    print(\"1. UEÎ≥Ñ Í∂§Ï†Å Í∑∏ÎûòÌîÑ...\")\n",
    "    plot_ue_trajectories(results_df, max_ues=6)\n",
    "    \n",
    "    # 2. Ïò§Ï∞® Î∂ÑÏÑù\n",
    "    print(\"2. Ïò§Ï∞® Î∂ÑÏÑù Í∑∏ÎûòÌîÑ...\")\n",
    "    plot_error_analysis(results_df)\n",
    "    \n",
    "    # 3. ÏòàÏ∏° vs Ïã§Ï†ú ÏÇ∞Ï†êÎèÑ\n",
    "    print(\"3. ÏòàÏ∏° vs Ïã§Ï†ú ÏÇ∞Ï†êÎèÑ...\")\n",
    "    plot_prediction_scatter(results_df)\n",
    "    \n",
    "    print(\"‚úÖ Î™®Îì† ÏãúÍ∞ÅÌôî ÏôÑÎ£å!\")\n",
    "\n",
    "# %%\n",
    "# Ïã§Ìñâ (Í∏∞Ï°¥ ÏΩîÎìú ÎßàÏßÄÎßâÏóê Ï∂îÍ∞Ä)\n",
    "plot_comprehensive_analysis(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
