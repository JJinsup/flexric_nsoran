#!/usr/bin/env python3
"""
PyTorch LSTM ê¸°ë°˜ ì‹¤ì‹œê°„ ëª¨ë“  UE ìœ„ì¹˜ ì¶”ì •
ğŸ§  ì‹œê³„ì—´ SINR ë°ì´í„°ë¥¼ í™œìš©í•œ ì •í™•í•œ ìœ„ì¹˜ ì˜ˆì¸¡

ì£¼ìš” íŠ¹ì§•:
- Bidirectional LSTM ëª¨ë¸ ì‚¬ìš©
- ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ (LOOKBACK=10)
- ì‹¤ì‹œê°„ ì²˜ë¦¬ ë° ì¦‰ì‹œ ì €ì¥
- ëª¨ë“  UE ë™ì‹œ ìœ„ì¹˜ ì¶”ì •
"""

import numpy as np
import torch
import torch.nn as nn
import joblib
import queue
import signal
import sys
import time
from collections import defaultdict, deque
from sklearn.preprocessing import MinMaxScaler

# utils.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ import
from utils import (
    SINRMeasurement, 
    DataParser,
    setup_logging, 
    create_socket_receiver, 
    start_receiver_thread
)

CONFIG = {
    'model_path': 'model/lstm_positioning.pth',           
    'scaler_x_path': 'model/lstm_x.pkl',      
    'scaler_y_path': 'model/lstm_y.pkl',      
    'output_file': 'lstm_trajectory.txt',
    'lookback': 10,  # ì‹œê³„ì—´ ìœˆë„ìš° í¬ê¸°
    'input_size': 12,  # ì…ë ¥ í”¼ì²˜ ê°œìˆ˜
    'hidden_size': 64,
    'output_size': 2,   # x, y ì¢Œí‘œ
    # ğŸ”¥ ì¶”ê°€: ë¬¼ë¦¬ì  ì œì•½ íŒŒë¼ë¯¸í„°
    'max_speed': 5.0,      # ìµœëŒ€ ì†ë„ (m/s)
    'time_interval': 0.1   # ì‹œê°„ ê°„ê²© (s)
}

class UELocalizationLSTM(nn.Module):
    """PyTorch LSTM ìœ„ì¹˜ ì¶”ì • ëª¨ë¸ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°)"""
    
    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=2):
        super(UELocalizationLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True, 
            bidirectional=True,
            dropout=0.1 if num_layers > 1 else 0
        )
        
        # Dense layers
        self.fc1 = nn.Linear(hidden_size * 2, 32)  # *2 for bidirectional
        self.fc2 = nn.Linear(32, output_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # LSTM forward
        lstm_out, _ = self.lstm(x)
        
        # ë§ˆì§€ë§‰ ì‹œì  ì¶œë ¥ ì‚¬ìš©
        last_output = lstm_out[:, -1, :]
        
        # Dense layers
        out = torch.relu(self.fc1(last_output))
        out = self.dropout(out)
        out = self.fc2(out)
        
        return out

class TimeSeriesBuffer:
    """LSTMìš© ì‹œê³„ì—´ ë°ì´í„° ë²„í¼"""
    
    def __init__(self, max_length: int = 10):
        self.max_length = max_length
        self.buffer = deque(maxlen=max_length)
        
    def add_features(self, features: np.ndarray):
        """ìƒˆë¡œìš´ íŠ¹ì„±ì„ ë²„í¼ì— ì¶”ê°€"""
        self.buffer.append(features.copy())
        
    def is_ready(self) -> bool:
        """ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ì¤€ë¹„ ì—¬ë¶€"""
        return len(self.buffer) == self.max_length
    
    def get_sequence(self) -> np.ndarray:
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°˜í™˜ [sequence_length, features]"""
        return np.array(list(self.buffer))
    
    def get_length(self) -> int:
        """í˜„ì¬ ë²„í¼ ê¸¸ì´"""
        return len(self.buffer)

class LSTMLocalizer:
    """LSTM ê¸°ë°˜ ì‹¤ì‹œê°„ ìœ„ì¹˜ ì¶”ì •ê¸°"""
    
    def __init__(self, model_path: str, scaler_x_path: str, scaler_y_path: str, output_file: str = "lstm_trajectory.txt"):
        print(f"ğŸ§  Initializing LSTM Localizer for ALL UEs...")
        
        # Device ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
        # LSTM ëª¨ë¸ ë¡œë“œ
        self.model = self._load_lstm_model(model_path)
        
        # Scaler ë¡œë“œ
        self.scaler_x = self._load_scaler(scaler_x_path, "Input")
        self.scaler_y = self._load_scaler(scaler_y_path, "Output")
        
        # ì¶œë ¥ íŒŒì¼ ì„¤ì •
        self.output_file = output_file
        self.file_handle = self._init_output_file()
        
        # UEë³„ ì‹œê³„ì—´ ë²„í¼ (LOOKBACK ê°œì˜ ê³¼ê±° ë°ì´í„° ì €ì¥)
        self.ue_buffers = defaultdict(lambda: TimeSeriesBuffer(CONFIG['lookback']))
        
        # UEë³„ ì²« timestamp ì¶”ì  (ìƒëŒ€ ì‹œê°„ ê³„ì‚°ìš©)
        self.ue_first_timestamps = {}
        # ğŸ”¥ ì¶”ê°€: CSV ë°°ì¹˜ ì½”ë“œì™€ ë™ì¼í•œ timestamp ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜ë“¤
        self.burst_group_mapping = {}      # burst_group -> sequence_timestamp ë§¤í•‘
        self.next_sequence_timestamp = 0.0 # ë‹¤ìŒ í• ë‹¹í•  sequence timestamp
        self.sequence_interval = 100.0     # 100ms ê°„ê²©
        # ğŸ”¥ ì¶”ê°€: UEë³„ ì´ì „ ìœ„ì¹˜ ì €ì¥ (ë¬¼ë¦¬ì  ì œì•½ìš©)
        self.ue_previous_positions = {}

        # í†µê³„ ë° ìƒíƒœ
        self.stats = {'processed': 0, 'estimated': 0, 'start_time': None}
        self.running = False

        # ğŸ”¥ ì¶”ê°€: ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë¡œë“œ
        try:
            self.category_mappings = joblib.load("category_mappings.pkl")
            print(f"âœ… Category mappings loaded:")
            for cat, mapping in self.category_mappings.items():
                print(f"  {cat}: {mapping}")
        except FileNotFoundError:
            print("âŒ category_mappings.pkl not found! Using default mapping")
            self.category_mappings = None

        print(f"ğŸ¯ LSTM Localizer ready for ALL UEs")
        print(f"ğŸ’¾ Output file: {self.output_file}")
        print(f"ğŸ”¢ Sequence length: {CONFIG['lookback']}")

    def _apply_category_mapping(self, value, category_name):
        """ì¹´í…Œê³ ë¦¬ ê°’ì„ í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ ë§¤í•‘"""
        if self.category_mappings and category_name in self.category_mappings:
            mapping = self.category_mappings[category_name]
            return mapping.get(value, 0)  # ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ê¸°ë³¸ê°’
        return value  # ë§¤í•‘ ì •ë³´ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©    
    
    def _init_output_file(self):
        """ì¶œë ¥ íŒŒì¼ ì´ˆê¸°í™”"""
        file_handle = open(self.output_file, 'w', buffering=1)
        file_handle.write("timestamp,imsi,x,y\n")
        file_handle.flush()
        print(f"ğŸ“ Output file initialized: {self.output_file}")
        return file_handle
    
    def _load_lstm_model(self, model_path: str):
        """PyTorch LSTM ëª¨ë¸ ë¡œë“œ"""
        try:
            # ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            # ëª¨ë¸ ìƒì„±
            model = UELocalizationLSTM(
                input_size=CONFIG['input_size'],
                hidden_size=CONFIG['hidden_size'],
                output_size=CONFIG['output_size']
            ).to(self.device)
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()  # ì¶”ë¡  ëª¨ë“œ
            
            print(f"âœ… LSTM model loaded from {model_path}")
            print(f"ğŸ§  Model parameters: {sum(p.numel() for p in model.parameters()):,}")
            
            return model
            
        except FileNotFoundError:
            print(f"âŒ Model file not found: {model_path}")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ Error loading LSTM model: {e}")
            sys.exit(1)
    
    def _load_scaler(self, scaler_path: str, scaler_type: str):
        """Scaler íŒŒì¼ ë¡œë“œ"""
        try:
            scaler = joblib.load(scaler_path)
            print(f"âœ… {scaler_type} scaler loaded from {scaler_path}")
            return scaler
        except FileNotFoundError:
            print(f"âŒ {scaler_type} scaler file not found: {scaler_path}")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ Error loading {scaler_type} scaler: {e}")
            sys.exit(1)
    
    def _extract_features(self, measurement: SINRMeasurement) -> np.ndarray:
        """ğŸ”¥ ê°„ì†Œí™”: sequence_timestamp ì§ì ‘ ì‚¬ìš©"""
        
        # 1. Burst group â†’ sequence timestamp
        timestamp_str = str(int(measurement.timestamp))
        burst_group = timestamp_str[:10]
        
        if burst_group not in self.burst_group_mapping:
            self.burst_group_mapping[burst_group] = self.next_sequence_timestamp
            print(f"ğŸ•’ New burst group '{burst_group}' â†’ {self.next_sequence_timestamp}ms")
            self.next_sequence_timestamp += 100.0
        
        sequence_timestamp = self.burst_group_mapping[burst_group]
        
        # ğŸ”¥ relative_timestamp ê³„ì‚° ì œê±°! ë°”ë¡œ sequence_timestamp ì‚¬ìš©
        ue_id_mapped = self._apply_category_mapping(measurement.ue_id, "ueImsiComplete")
        serving_cell_mapped = self._apply_category_mapping(measurement.serving_cell_id, "L3 serving Id(m_cellId)")
        neigh1_mapped = self._apply_category_mapping(measurement.neighbor1_id, "L3 neigh Id 1 (cellId)")
        neigh2_mapped = self._apply_category_mapping(measurement.neighbor2_id, "L3 neigh Id 2 (cellId)")
        neigh3_mapped = self._apply_category_mapping(measurement.neighbor3_id, "L3 neigh Id 3 (cellId)")
        
        features = [
            sequence_timestamp,  # ğŸ”¥ ë°”ë¡œ ì‚¬ìš©! relative ê³„ì‚° ì•ˆ í•¨
            ue_id_mapped,
            serving_cell_mapped,
            measurement.serving_cell_sinr,
            neigh1_mapped,
            measurement.neighbor1_sinr,
            neigh2_mapped,
            measurement.neighbor2_sinr,
            neigh3_mapped,
            measurement.neighbor3_sinr,
            measurement.serving_cell_x,
            measurement.serving_cell_y
        ]
        
        return np.array(features, dtype=np.float32)
    
    def apply_movement_constraint(self, pred_pos: np.ndarray, prev_pos: np.ndarray, 
                            max_speed: float = 5.0, dt: float = 0.1) -> np.ndarray:

        max_distance = max_speed * dt  # ìµœëŒ€ ì´ë™ê±°ë¦¬ (ì˜ˆ: 5m/s * 0.1s = 0.5m)
        
        # ì´ë™ ë²¡í„° ê³„ì‚°
        movement = pred_pos - prev_pos
        distance = np.linalg.norm(movement)
        
        # ìµœëŒ€ ì´ë™ê±°ë¦¬ ì´ˆê³¼ ì‹œ ì œí•œ
        if distance > max_distance:
            # ë°©í–¥ì€ ìœ ì§€í•˜ë˜ ê±°ë¦¬ë§Œ ì œí•œ
            movement = movement * (max_distance / distance)
            constrained_pos = prev_pos + movement
            
            print(f"âš ï¸  Movement constrained: {distance:.2f}m â†’ {max_distance:.2f}m")
            return constrained_pos
        
        return pred_pos

    def process_measurement(self, measurement: SINRMeasurement):
        """ğŸ§  LSTMìœ¼ë¡œ ìœ„ì¹˜ ì¶”ì • (ë¬¼ë¦¬ì  ì œì•½ ì ìš©)"""
        try:
            # 1. íŠ¹ì„± ì¶”ì¶œ
            features = self._extract_features(measurement)
            
            # 2. UEë³„ ì‹œê³„ì—´ ë²„í¼ì— ì¶”ê°€
            buffer = self.ue_buffers[measurement.ue_id]
            buffer.add_features(features)
            
            # 3. ì¶©ë¶„í•œ ì‹œê³„ì—´ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì˜ˆì¸¡
            if buffer.is_ready():
                # ì‹œê³„ì—´ ë°°ì—´ ìƒì„± [LOOKBACK, Features]
                sequence = buffer.get_sequence()
                
                # ì…ë ¥ ì •ê·œí™”
                sequence_scaled = self.scaler_x.transform(sequence.reshape(-1, CONFIG['input_size'])).reshape(sequence.shape)
                
                # PyTorch í…ì„œë¡œ ë³€í™˜ [1, LOOKBACK, Features]
                sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(self.device)
                
                # LSTM ì˜ˆì¸¡
                with torch.no_grad():
                    prediction_scaled = self.model(sequence_tensor)
                
                # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
                prediction_scaled_np = prediction_scaled.cpu().numpy()
                
                # ì¶œë ¥ ì—­ì •ê·œí™”
                raw_prediction = self.scaler_y.inverse_transform(prediction_scaled_np)[0]
                
                # ğŸ”¥ ì¶”ê°€: ë¬¼ë¦¬ì  ì œì•½ ì¡°ê±´ ì ìš©
                if measurement.ue_id in self.ue_previous_positions:
                    prev_pos = self.ue_previous_positions[measurement.ue_id]
                    constrained_prediction = self.apply_movement_constraint(raw_prediction, prev_pos)
                else:
                    # ì²« ë²ˆì§¸ ì˜ˆì¸¡ì´ë©´ ì œì•½ ì—†ì´ ì‚¬ìš©
                    constrained_prediction = raw_prediction
                
                # ğŸ”¥ ì¶”ê°€: ì´ì „ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
                self.ue_previous_positions[measurement.ue_id] = constrained_prediction.copy()
                
                # x, y ì¢Œí‘œ ì¶”ì¶œ
                x, y = constrained_prediction

                # íŒŒì¼ì— ì¦‰ì‹œ ì €ì¥
                self.file_handle.write(f"{measurement.timestamp},{measurement.ue_id},{x:.6f},{y:.6f}\n")
                
                self.stats['estimated'] += 1
                
                # ì£¼ê¸°ì  flush ë° ë””ë²„ê¹…
                if self.stats['estimated'] % 50 == 0:
                    self.file_handle.flush()
                    print(f"ğŸ’¾ {self.stats['estimated']} positions saved to {self.output_file}")
                
                if self.stats['estimated'] % 10 == 0:
                    print(f"ğŸ§  UE_{measurement.ue_id}: ({x:.2f},{y:.2f}) | "
                        f"SeqTime:{self.current_sequence_timestamp}ms | "  # ğŸ”¥ ì €ì¥ëœ ê°’ ì‚¬ìš©
                        f"SINR: [{measurement.serving_cell_sinr:.1f}, {measurement.neighbor1_sinr:.1f}] | "
                        f"Buffer: {buffer.get_length()}/{CONFIG['lookback']}")
                
            else:
                # ë²„í¼ ì±„ìš°ëŠ” ì¤‘
                if buffer.get_length() % 5 == 0:
                    print(f"ğŸ“Š UE_{measurement.ue_id}: Buffering {buffer.get_length()}/{CONFIG['lookback']}...")
                    
        except Exception as e:
            print(f"âŒ LSTM prediction error for UE {measurement.ue_id}: {e}")
    
    def start_data_collection(self, data_queue: queue.Queue):
        """ğŸ”¥ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ëª¨ë“  UE ì²˜ë¦¬"""
        self.running = True
        self.stats['start_time'] = time.time()
        print(f"ğŸš€ Starting real-time LSTM localization for ALL UEs")
        print(f"â±ï¸  Sequence buffer size: {CONFIG['lookback']}")
        
        while self.running:
            try:
                line = data_queue.get(timeout=0.1)
                measurement = DataParser.parse_sinr_line(line)
                
                if measurement:  # ëª¨ë“  UE ì²˜ë¦¬
                    self.stats['processed'] += 1
                    
                    # ëª¨ë“  UEì— ëŒ€í•´ LSTM ìœ„ì¹˜ ì¶”ì •
                    self.process_measurement(measurement)
                    
                    # ì£¼ê¸°ì  ìƒíƒœ ì¶œë ¥
                    if self.stats['processed'] % 100 == 0:
                        self._print_status()
                        
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ Real-time processing error: {e}")
    
    def _print_status(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.stats['start_time'] if self.stats['start_time'] else 0
        processing_rate = self.stats['processed'] / elapsed if elapsed > 0 else 0
        success_rate = (self.stats['estimated'] / self.stats['processed']) * 100 if self.stats['processed'] > 0 else 0
        unique_ues = len(self.ue_first_timestamps)
        active_buffers = sum(1 for buffer in self.ue_buffers.values() if buffer.is_ready())
        # ğŸ”¥ ì¶”ê°€: timestamp ë§¤í•‘ ìƒíƒœ
        burst_groups = len(self.burst_group_mapping)

        print(f"ğŸ“Š LSTM ALL UEs | "
              f"Rate:{processing_rate:.1f}/s | "
              f"Success:{success_rate:.1f}% | "
              f"Processed:{self.stats['processed']} | "
              f"Estimated:{self.stats['estimated']} | "
              f"UEs:{unique_ues} | "
              f"Ready:{active_buffers}")
    
    def stop(self):
        """ì •ì§€ ë° íŒŒì¼ ì•ˆì „ ì¢…ë£Œ"""
        print(f"\nğŸ›‘ Stopping LSTM localization...")
        self.running = False
        
        # ìµœì¢… í†µê³„
        self._print_status()
        print(f"\nğŸ“ˆ Final UE Buffer Status:")
        for ue_id, buffer in self.ue_buffers.items():
            status = buffer.get_length()
            ready = "âœ… Ready" if buffer.is_ready() else "â³ Buffering"
            print(f"   UE_{ue_id}: {status}/{CONFIG['lookback']} measurements - {ready}")
        
        # íŒŒì¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
        if self.file_handle:
            self.file_handle.flush()
            self.file_handle.close()
            print(f"âœ… Output file closed: {self.output_file} ({self.stats['estimated']} total positions)")
        
        print(f"âœ… LSTM localization completed")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    
    print("=" * 60)
    print("ğŸ§  PyTorch LSTM-Based Real-time ALL UEs Localization")
    print("ğŸ”„ Time-series SINR pattern learning")
    print("=" * 60)
    print(f"ğŸ¯ Target: ALL UEs (no filtering)")
    print(f"ğŸ§  Model: {CONFIG['model_path']}")
    print(f"ğŸ“Š Input scaler: {CONFIG['scaler_x_path']}")
    print(f"ğŸ“Š Output scaler: {CONFIG['scaler_y_path']}")
    print(f"ğŸ’¾ Output file: {CONFIG['output_file']}")
    print(f"ğŸ”¢ Sequence length: {CONFIG['lookback']} timesteps")
    print("=" * 60)
    
    # LSTM ìœ„ì¹˜ ì¶”ì •ê¸° ì´ˆê¸°í™”
    try:
        localizer = LSTMLocalizer(
            model_path=CONFIG['model_path'],
            scaler_x_path=CONFIG['scaler_x_path'],
            scaler_y_path=CONFIG['scaler_y_path'],
            output_file=CONFIG['output_file']
        )
    except Exception as e:
        print(f"âŒ LSTM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì‹  ì„¤ì • (utils.py í™œìš©)
    data_queue = queue.Queue()
    receiver = create_socket_receiver(data_queue)
    receiver_thread = start_receiver_thread(receiver)
    
    # ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬
    def signal_handler(sig, frame):
        print(f"\nğŸ›‘ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹  ({sig})")
        localizer.stop()
        receiver.stop()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    print("ğŸš€ LSTM localization started for ALL UEs!")
    print("ğŸ’¡ C xAppì„ ì‹¤í–‰í•´ì„œ SINR ë°ì´í„°ë¥¼ ì „ì†¡í•˜ì„¸ìš”")
    print("ğŸ”„ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµìœ¼ë¡œ ì •í™•í•œ ìœ„ì¹˜ ì˜ˆì¸¡!")
    print(f"â±ï¸  {CONFIG['lookback']}ê°œ ì¸¡ì •ê°’ì´ ëª¨ì´ë©´ ì˜ˆì¸¡ ì‹œì‘")
    print("ğŸ›‘ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
    print("-" * 60)
    
    try:
        # ğŸ§  ì‹¤ì‹œê°„ LSTM ìœ„ì¹˜ ì¶”ì • ì‹œì‘
        localizer.start_data_collection(data_queue)
    except KeyboardInterrupt:
        pass
    finally:
        print("\nğŸ›‘ ì •ë¦¬ ì¤‘...")
        localizer.stop()
        receiver.stop()
        print("âœ… LSTM localization ì™„ë£Œ!")

if __name__ == "__main__":
    print("ğŸ” Checking required files...")
    
    required_files = [
        CONFIG['model_path'],
        CONFIG['scaler_x_path'], 
        CONFIG['scaler_y_path']
    ]
    
    missing_files = []
    for file_path in required_files:
        try:
            with open(file_path, 'rb'):
                pass
            print(f"  âœ… {file_path}")
        except FileNotFoundError:
            missing_files.append(file_path)
            print(f"  âŒ {file_path}")
    
    if missing_files:
        print(f"\nâŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤:")
        for file in missing_files:
            print(f"   - {file}")
        print(f"\nğŸ’¡ ë¨¼ì € LSTM ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”!")
        sys.exit(1)
    
    print("âœ… All files found! Starting LSTM localization...\n")
    main()
